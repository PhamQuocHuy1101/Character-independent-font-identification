{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcaf9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "97784b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class CropSize(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(CropSize, self).__init__()\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self, image):\n",
    "        '''\n",
    "            image: tensor image\n",
    "        '''\n",
    "        h, w = image.shape[-2:]\n",
    "        delta_x = self.size - w\n",
    "        delta_y = self.size - h\n",
    "        left = int(delta_x / 2)\n",
    "        right = delta_x - left\n",
    "        top = int(delta_y / 2)\n",
    "        bottom = delta_y - top\n",
    "        print(image.shape)\n",
    "        out = functional.pad(image, [left, right, top, bottom], mode='replicate')\n",
    "        print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'Padding with width x height: {self.size} x {self.size}'\n",
    "\n",
    "to_tensor = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    CropSize(64),\n",
    "    T.Normalize(mean=[0.9, 0.9, 0.9], std=[0.1, 0.1, 0.1]),\n",
    "])\n",
    "\n",
    "augmenter = T.Compose([\n",
    "    T.ColorJitter(0.2, 0.1, 0.5, 0.2),\n",
    "    T.RandomGrayscale(0.2),\n",
    "    to_tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f32247cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 40, 64])\n",
      "torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "aimg = to_tensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cfa5fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAAAoCAIAAADBrGu+AAAB30lEQVR4nO2ZrcvyUBiHfxPcHIIfoIg228AgFsFqUhgG7TaDCGLS/8RisWqw+QXOWYwOLZpMBpnBONgMe8LK8+LjW977vOOBc7Wde5xrF2McQcF1XfxmAn4/wL/CA/yGB/gND/AbyoDVaiUIgiAIx+ORcNu/w9+A3/AAv+EBfsMDPrPb7RqNRjqdlmVZUZRer3e9Xuk1Lh3L5dLb0zCMbrf77pIkaTweExpd12USUK1WARQKhclkcj6f9/v9YDAIh8MAAoHAer0mlDIJAKCqqm3b36eGYcTjcQDZbNZxHCopq4DH4/F+w2g08qbT6ZRKyuQjrtfryWTyfb3ZbEYiEQDz+ZzKxSrgx3VRFMvlMoDT6UTlYhKQy+U+jTKZDID7/U7l+t8HWSwWA2DbNtWGTAIcx/k0ej6fALwvgQQmAZqmfRrdbjcAqVSKysUkYLvd/rhuWZau6wBKpRKVi0nAZrMxTfN9fTgcWpYFQFVVMhnVgeL+eZDVarXX6/V9qmmaLMsA8vk8oZRJQKvVAlAsFmez2eVy0XW93W4Hg0EAoigeDgdCKZMAy7I6nc77245Go7S/5FwWAZIkeZeLxaJSqSQSiVAopChKv983TZNQ5yG4/A8Of+EBfsMD/ObXB3wBcD11zemPipcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x40 at 0x7F33F126B070>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('../../../dataset/font_images_all/trispace/light/b_trispace_light.png').convert('RGB')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbddc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
